{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 自然言語処理入門"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】BoWのスクラッチ実装\n",
    "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"This movie is SOOOO funny!!!\", \"What a movie! I never\", \"best movie ever!!!!! this movie\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word_sentence = []\n",
    "splited_word = []\n",
    "for d in data:\n",
    "    a = d.split()\n",
    "    data_word_sentence.append(a)\n",
    "    for i in a:\n",
    "        splited_word.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_word = list(set(splited_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'movie', 'is', 'SOOOO', 'funny!!!'],\n",
       " ['What', 'a', 'movie!', 'I', 'never'],\n",
       " ['best', 'movie', 'ever!!!!!', 'this', 'movie']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ever!!!!!',\n",
       " 'movie',\n",
       " 'What',\n",
       " 'is',\n",
       " 'a',\n",
       " 'never',\n",
       " 'best',\n",
       " 'this',\n",
       " 'movie!',\n",
       " 'I',\n",
       " 'SOOOO',\n",
       " 'This',\n",
       " 'funny!!!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict1 = {}\n",
    "for i in splited_word:\n",
    "    one_list = []\n",
    "    for j in data_word_sentence:\n",
    "        one_list.append(j.count(i))\n",
    "        result_dict1[i] = one_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ever!!!!!': [0, 0, 1],\n",
       " 'movie': [1, 0, 2],\n",
       " 'What': [0, 1, 0],\n",
       " 'is': [1, 0, 0],\n",
       " 'a': [0, 1, 0],\n",
       " 'never': [0, 1, 0],\n",
       " 'best': [0, 0, 1],\n",
       " 'this': [0, 0, 1],\n",
       " 'movie!': [0, 1, 0],\n",
       " 'I': [0, 1, 0],\n",
       " 'SOOOO': [1, 0, 0],\n",
       " 'This': [1, 0, 0],\n",
       " 'funny!!!': [1, 0, 0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result_dict1, index=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ever!!!!!</th>\n",
       "      <th>movie</th>\n",
       "      <th>What</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>this</th>\n",
       "      <th>movie!</th>\n",
       "      <th>I</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>This</th>\n",
       "      <th>funny!!!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This movie is SOOOO funny!!!</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What a movie! I never</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best movie ever!!!!! this movie</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ever!!!!!  movie  What  is  a  never  best  \\\n",
       "This movie is SOOOO funny!!!             0      1     0   1  0      0     0   \n",
       "What a movie! I never                    0      0     1   0  1      1     0   \n",
       "best movie ever!!!!! this movie          1      2     0   0  0      0     1   \n",
       "\n",
       "                                 this  movie!  I  SOOOO  This  funny!!!  \n",
       "This movie is SOOOO funny!!!        0       0  0      1     1         1  \n",
       "What a movie! I never               0       1  1      0     0         0  \n",
       "best movie ever!!!!! this movie     1       0  0      0     0         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_word_2gram = []\n",
    "word_sentence_2gram = []\n",
    "for l in data_word_sentence:\n",
    "    wow = []\n",
    "    for idx in range(len(l) -2 +1):\n",
    "        ab = l[idx:idx + 2]\n",
    "        d = ab[0] +\" \" + ab[1]\n",
    "        split_word_2gram.append(d)\n",
    "        wow.append(d)\n",
    "    \n",
    "    word_sentence_2gram.append(wow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This movie', 'movie is', 'is SOOOO', 'SOOOO funny!!!'],\n",
       " ['What a', 'a movie!', 'movie! I', 'I never'],\n",
       " ['best movie', 'movie ever!!!!!', 'ever!!!!! this', 'this movie']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentence_2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie',\n",
       " 'movie is',\n",
       " 'is SOOOO',\n",
       " 'SOOOO funny!!!',\n",
       " 'What a',\n",
       " 'a movie!',\n",
       " 'movie! I',\n",
       " 'I never',\n",
       " 'best movie',\n",
       " 'movie ever!!!!!',\n",
       " 'ever!!!!! this',\n",
       " 'this movie']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_word_2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict2 = {}\n",
    "for i in split_word_2gram:\n",
    "    one_list2 = []\n",
    "    for j in word_sentence_2gram:\n",
    "        one_list2.append(j.count(i))\n",
    "        result_dict2[i] = one_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This movie': [1, 0, 0],\n",
       " 'movie is': [1, 0, 0],\n",
       " 'is SOOOO': [1, 0, 0],\n",
       " 'SOOOO funny!!!': [1, 0, 0],\n",
       " 'What a': [0, 1, 0],\n",
       " 'a movie!': [0, 1, 0],\n",
       " 'movie! I': [0, 1, 0],\n",
       " 'I never': [0, 1, 0],\n",
       " 'best movie': [0, 0, 1],\n",
       " 'movie ever!!!!!': [0, 0, 1],\n",
       " 'ever!!!!! this': [0, 0, 1],\n",
       " 'this movie': [0, 0, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df2 = pd.DataFrame(result_dict2, index=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is SOOOO</th>\n",
       "      <th>SOOOO funny!!!</th>\n",
       "      <th>What a</th>\n",
       "      <th>a movie!</th>\n",
       "      <th>movie! I</th>\n",
       "      <th>I never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever!!!!!</th>\n",
       "      <th>ever!!!!! this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This movie is SOOOO funny!!!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What a movie! I never</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best movie ever!!!!! this movie</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 This movie  movie is  is SOOOO  \\\n",
       "This movie is SOOOO funny!!!              1         1         1   \n",
       "What a movie! I never                     0         0         0   \n",
       "best movie ever!!!!! this movie           0         0         0   \n",
       "\n",
       "                                 SOOOO funny!!!  What a  a movie!  movie! I  \\\n",
       "This movie is SOOOO funny!!!                  1       0         0         0   \n",
       "What a movie! I never                         0       1         1         1   \n",
       "best movie ever!!!!! this movie               0       0         0         0   \n",
       "\n",
       "                                 I never  best movie  movie ever!!!!!  \\\n",
       "This movie is SOOOO funny!!!           0           0                0   \n",
       "What a movie! I never                  1           0                0   \n",
       "best movie ever!!!!! this movie        0           1                1   \n",
       "\n",
       "                                 ever!!!!! this  this movie  \n",
       "This movie is SOOOO funny!!!                  0           0  \n",
       "What a movie! I never                         0           0  \n",
       "best movie ever!!!!! this movie               1           1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】TF-IDFの計算\n",
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
    "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/morishuuya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 5000)\n",
    "\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = vectorizer.fit_transform(x_test)\n",
    "\n",
    "print(x_train_tfidf.shape)\n",
    "print(x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 5000, ngram_range=(2,2))\n",
    "\n",
    "x_train_tfidf_2gram = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf_2gram = vectorizer.fit_transform(x_test)\n",
    "\n",
    "print(x_train_tfidf_2gram.shape)\n",
    "print(x_test_tfidf_2gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1000)\n",
      "(25000, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 1000, ngram_range=(2,2))\n",
    "\n",
    "x_train_tfidf_1000 = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf_1000 = vectorizer.fit_transform(x_test)\n",
    "\n",
    "print(x_train_tfidf_1000.shape)\n",
    "print(x_test_tfidf_1000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】TF-IDFを用いた学習\n",
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lgrg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morishuuya/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55672\n"
     ]
    }
   ],
   "source": [
    "lgrg.fit(x_train_tfidf, y_train)\n",
    "\n",
    "y_pred = lgrg.predict(x_test_tfidf)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56448\n"
     ]
    }
   ],
   "source": [
    "lgrg.fit(x_train_tfidf_2gram, y_train)\n",
    "\n",
    "y_pred = lgrg.predict(x_test_tfidf_2gram)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56576\n"
     ]
    }
   ],
   "source": [
    "lgrg.fit(x_train_tfidf_1000, y_train)\n",
    "\n",
    "y_pred = lgrg.predict(x_test_tfidf_1000)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】TF-IDFのスクラッチ実装\n",
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。  \n",
    "This movie is SOOOO funny!!!  \n",
    "What a movie! I never  \n",
    "best movie ever!!!!! this movie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal\n",
    "tfidf_array = np.zeros((3, 13))\n",
    "for j in range(3):\n",
    "    for i in range(13):\n",
    "        n = result_df.iat[j, i]\n",
    "        s = result_df.sum(axis=1)[j]\n",
    "        N = result_df.shape[0]\n",
    "        d = (result_df.iloc[:,i] >= 1).sum()\n",
    "        \n",
    "        tf = n / s\n",
    "        idf = np.log(N / d)\n",
    "        \n",
    "        tf_idf = tf * idf\n",
    "        tfidf_array[j ,i] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.08109302, 0.        , 0.21972246, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.21972246, 0.21972246, 0.21972246],\n",
       "       [0.        , 0.        , 0.21972246, 0.        , 0.21972246,\n",
       "        0.21972246, 0.        , 0.        , 0.21972246, 0.21972246,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.21972246, 0.16218604, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.21972246, 0.21972246, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "tfidf_sk_arr = np.zeros((3, 13))\n",
    "for j in range(3):\n",
    "    for i in range(13):\n",
    "        n = result_df.iat[j, i]\n",
    "        s = result_df.sum(axis=1)[j]\n",
    "        N = result_df.shape[0]\n",
    "        d = (result_df.iloc[:,i] >= 1).sum()\n",
    "        \n",
    "        tf = n \n",
    "        idf = np.log((1 + N) / (1 +  d)) + 1\n",
    "        \n",
    "        tf_idf = tf * idf\n",
    "        tfidf_sk_arr[j ,i] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.28768207, 0.        , 1.69314718, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.69314718, 1.69314718, 1.69314718],\n",
       "       [0.        , 0.        , 1.69314718, 0.        , 1.69314718,\n",
       "        1.69314718, 0.        , 0.        , 1.69314718, 1.69314718,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [1.69314718, 2.57536414, 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.69314718, 1.69314718, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sk_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[ 0.0082302   0.02408049  0.03856738  0.03244873  0.03135805  0.03467129\n",
      " -0.02705073  0.00499627  0.03091267 -0.04548453]\n",
      "movieのベクトル : \n",
      "[ 0.04145413  0.03885274  0.03308154 -0.04712112 -0.0098867   0.03372115\n",
      " -0.03454908  0.02594179  0.00662227 -0.02679704]\n",
      "isのベクトル : \n",
      "[ 0.03406468 -0.00492987  0.03458171 -0.01776264 -0.03222978 -0.013404\n",
      "  0.03543762  0.02811564 -0.0360048  -0.02668507]\n",
      "veryのベクトル : \n",
      "[-0.02968424 -0.01522698 -0.01072035  0.01340295 -0.01623585 -0.03185401\n",
      " -0.02299884  0.02275814  0.01447068 -0.04376133]\n",
      "goodのベクトル : \n",
      "[ 0.01515114 -0.00278226  0.02906049 -0.04896694  0.04901393  0.04116394\n",
      "  0.02269715 -0.04006554 -0.03310281 -0.04751993]\n",
      "filmのベクトル : \n",
      "[-0.03428271 -0.03327842 -0.01789581 -0.00411011  0.04424705 -0.02275743\n",
      "  0.00906097 -0.04885813 -0.0030917  -0.01173164]\n",
      "aのベクトル : \n",
      "[-0.04007009 -0.03319386 -0.02490124  0.04479393  0.03016923  0.03466148\n",
      "  0.04884226  0.0185284  -0.04004807  0.04975155]\n",
      "badのベクトル : \n",
      "[-0.00403989 -0.00295711  0.03996669 -0.0205042   0.03459535 -0.01472872\n",
      "  0.0191093  -0.01589637  0.01071609  0.0240677 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morishuuya/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "\n",
    "for vocab in model.wv.vocab.keys():\n",
    "    print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morishuuya/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocabs = model.wv.vocab.keys()\n",
    "\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】コーパスの前処理\n",
    "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupus = []\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = x_train[i].lower()\n",
    "    x_train[i] = x_train[i].replace(\"!\", \"\").strip()\n",
    "    x_train[i] = x_train[i].replace(\"?\", \"\").strip()\n",
    "    x_train[i] = x_train[i].replace(\"<br />\", \"\").strip()\n",
    "    x_train[i] = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\", \"\", x_train[i])\n",
    "    x_train[i] = x_train[i].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】Word2Vecの学習\n",
    "Word2Vecの学習を行なってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morishuuya/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22228092, 28707255)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = x_train\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】ベクトルの可視化\n",
    "得られたベクトルをt-SNEにより可視化してください。また、いくつかの単語を選びwv.most_similarを用いて似ている単語を調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.9663302302360535),\n",
       " ('bad', 0.953368604183197),\n",
       " ('horrible', 0.9522203207015991)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('documentarian.', 0.9650168418884277),\n",
       " ('overlook:', 0.9612120985984802),\n",
       " ('futilely', 0.9404004812240601)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"like\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('on.one', 0.9569797515869141),\n",
       " ('other.and', 0.9515284299850464),\n",
       " ('get\",', 0.9365780353546143)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"by\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.manifold import TSNE\\nimport matplotlib.pyplot as plt\\n\\nvocabs = model.wv.vocab.keys()\\n\\ntsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=250, random_state=23, verbose=1)\\n\\nvectors_tsne = tsne_model.fit_transform(model[vocabs])\\n\\n\\nfig, ax = plt.subplots(figsize=(5,5))\\nax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\\nfor i, word in enumerate(list(vocabs)):\\n    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\\nax.set_yticklabels([])\\nax.set_xticklabels([])\\nplt.show()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocabs = model.wv.vocab.keys()\n",
    "\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=250, random_state=23, verbose=1)\n",
    "\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "処理に時間がかかるため、可視化はやりませんでした。（遠藤さんに了承済み）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
